{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c3f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc945fbd300>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bs4/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/andres/anaconda3/lib/python3.10/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andres/anaconda3/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=1345c083723877f17033173e3852ace590829cabd9e0bdb36bb8bc28f8554573\n",
      "  Stored in directory: /home/andres/.cache/pip/wheels/e4/62/1d/d4d1bc4f33350ff84227f89b258edb552d604138e3739f5c83\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m930.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m886.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n",
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m674.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/andres/anaconda3/lib/python3.10/site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: packaging in /home/andres/anaconda3/lib/python3.10/site-packages (from keras-tuner) (22.0)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests->keras-tuner) (2022.12.7)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install contractions\n",
    "!pip install scikit-multilearn\n",
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7286a04-05bc-4eca-80db-37066e74a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m852.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m980.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/andres/.local/lib/python3.10/site-packages (from spacy) (1.10.7)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: jinja2 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.3/913.3 kB\u001b[0m \u001b[31m716.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m833.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: setuptools in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/andres/.local/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (5.2.1)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m492.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m476.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/andres/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andres/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b19c49-f341-41c4-acd6-485ce433bbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-01 11:45:00.266636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: jinja2 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/andres/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/andres/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/andres/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/andres/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andres/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andres/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andres/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andres/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3e1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cutting-stability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43130/3693549209.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  dataFrameTrain['clear'] = (dataFrameTrain[dataFrameTrain.columns[1:7]].max(axis=1) == 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "dataFrameTrain = pd.read_csv('../Data/train.csv')\n",
    "dataFrameTrain['clear'] = (dataFrameTrain[dataFrameTrain.columns[1:7]].max(axis=1) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mineral-likelihood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  clear  \n",
       "0           0             0        0       0       0              0      1  \n",
       "1           0             0        0       0       0              0      1  \n",
       "2           0             0        0       0       0              0      1  \n",
       "3           0             0        0       0       0              0      1  \n",
       "4           0             0        0       0       0              0      1  \n",
       "...       ...           ...      ...     ...     ...            ...    ...  \n",
       "159566      0             0        0       0       0              0      1  \n",
       "159567      0             0        0       0       0              0      1  \n",
       "159568      0             0        0       0       0              0      1  \n",
       "159569      0             0        0       0       0              0      1  \n",
       "159570      0             0        0       0       0              0      1  \n",
       "\n",
       "[159571 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrameTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alpine-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text normalization\n",
    "import re\n",
    "pd.options.mode.chained_assignment = None\n",
    "dataFrameTrain['comment_text'] = dataFrameTrain['comment_text'].apply(lambda x: x.lower())\n",
    "dataFrameTrain['comment_text'] = dataFrameTrain['comment_text'].apply((lambda x:re.sub('@[^\\s]+','',x)))\n",
    "dataFrameTrain['comment_text'] = dataFrameTrain['comment_text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ignored-engineer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    explain = []\n",
    "    lemma_stop = []\n",
    "    pos_stop = []\n",
    "    explain_stop = []\n",
    "   \n",
    "    for word in sp(text):     \n",
    "        if not word.is_punct and word.text != ' ' and word.pos_ != 'SPACE':\n",
    "            lemma.append(word.lemma_)\n",
    "            pos.append(word.pos_)\n",
    "            explain.append(spacy.explain(word.tag_))\n",
    "            if not word.text in spacy_stopwords:\n",
    "                lemma_stop.append(word.lemma_)\n",
    "                pos_stop.append(word.pos_)\n",
    "                explain_stop.append(spacy.explain(word.tag_))\n",
    "                \n",
    "            \n",
    "    return lemma, pos, explain, lemma_stop, pos_stop, explain_stop\n",
    "            \n",
    "    return tokens\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = contractions.fix(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text) # clean url\n",
    "    text = re.sub(r'#(\\w+)', '', text)   # clean hashes\n",
    "    text = re.sub(r'@(\\w+)', '', text)   # clean @\n",
    "    text = re.sub(r'\\d+', '', text)      # clean digits\n",
    "    text = re.sub(r'[,!@\\'\\\"?\\.$%_&#*+-:;]', '', text)   # clean punctuation\n",
    "    text = remove_accented_chars(text)\n",
    "    text = strip_html_tags(text)\n",
    "    \n",
    "    return text    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa0fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andres/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/home/andres/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lemma_s = []\n",
    "pos_s = []\n",
    "explain_s = []\n",
    "lemma_stop_s = []\n",
    "pos_stop_s = []\n",
    "explain_stop_s = []\n",
    "ids_rem = []\n",
    "count = 1\n",
    "for i, e in zip(dataFrameTrain['comment_text'], range(dataFrameTrain['comment_text'].shape[0])):  \n",
    "    if count % 20000 == 0:\n",
    "        print(count)\n",
    "    text  =  clean_text(i)\n",
    "    \n",
    "    lemma, pos, explain, lemma_stop, pos_stop, explain_stop = tokenize(text)\n",
    "    if lemma != []:\n",
    "        lemma_s.append(lemma)\n",
    "        pos_s.append(pos)\n",
    "        explain_s.append(explain)\n",
    "        \n",
    "    if lemma_stop != []:\n",
    "        lemma_stop_s.append(lemma_stop)\n",
    "        pos_stop_s.append(pos_stop)\n",
    "        explain_stop_s.append(explain_stop)\n",
    "    else:\n",
    "        dataFrameTrain.drop(e, inplace=True)    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/processed/sentences_lemma.data\", 'wb') as filehandle:\n",
    "    pickle.dump(lemma_s, filehandle)\n",
    "with open(\"../Data/processed/sentences_pos.data\", 'wb') as filehandle:\n",
    "    pickle.dump(pos_s, filehandle)\n",
    "with open(\"../Data/processed/sentences_explain.data\", 'wb') as filehandle:\n",
    "    pickle.dump(explain_s, filehandle)\n",
    "with open(\"../Data/processed/sentences_lemma_stop.data\", 'wb') as filehandle:\n",
    "    pickle.dump(lemma_stop_s, filehandle)\n",
    "with open(\"../Data/processed/sentences_pos_stop.data\", 'wb') as filehandle:\n",
    "    pickle.dump(pos_stop_s, filehandle)\n",
    "with open(\"../Data/processed/sentences_explain_stop.data\", 'wb') as filehandle:\n",
    "    pickle.dump(explain_stop_s, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTrain['lemma_stop'] = lemma_stop_s\n",
    "dataFrameTrain['pos_stop'] = pos_stop_s\n",
    "dataFrameTrain['explain_stop'] = explain_stop_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51541e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTrain.to_csv('../Data/processed/tokenized_stop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = pd.read_csv('../Data/processed/tokenized_stop.csv')\n",
    "tokenized['lemma_stop'] = tokenized['lemma_stop'].apply(eval)\n",
    "tokenized['pos_stop'] = tokenized['pos_stop'].apply(eval)\n",
    "tokenized['explain_stop'] = tokenized['explain_stop'].apply(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7bbd5-3b05-480e-873f-fd3a8ce1f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS = 20000\n",
    "max_len = 200\n",
    "X = tokenized['lemma_stop'].values\n",
    "y = tokenized[tokenized.columns[2:8]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33324d91-407d-429a-a42c-090546f03ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "X_train, y_train, X_test, y_test = iterative_train_test_split(X.reshape(-1,1), y, test_size = 0.2)\n",
    "X_train = np.squeeze(X_train)\n",
    "X_test = np.squeeze(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7b1a1-dd82-4b75-b5bd-c1e35035527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = pad_sequences(X_train, padding='pre', truncating='pre', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='pre', truncating='pre', maxlen=max_len)\n",
    "joblib.dump(tokenizer, 'Tokenizer/tok.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430831c3-e07b-4914-894e-33a7768ef801",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12c468-5fd6-4618-8c28-6cd4bebc53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['X'] = X_train.tolist()\n",
    "train['y'] = y_train.tolist()\n",
    "test = pd.DataFrame()\n",
    "test['X'] = X_test.tolist()\n",
    "test['y'] = y_test.tolist()\n",
    "train.to_csv('../Data/processed/train.csv', index=False)\n",
    "test.to_csv('../Data/processed/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84b811-5cdc-4700-8e5e-15de4e59665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b60ab-32f7-48d6-bf09-f80245297bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
